{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5.5 Challenge: what model can answer this question?\n",
    "For each of the problems below identify which supervised learning methods would be best for that particular problem.\n",
    "Explain Reasoning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics:\n",
    "\n",
    "What do I need the model to do?\n",
    "* predicting continuous variable\n",
    "\n",
    "\n",
    "I would use a **multivariate linear regression** since the target variable (the runners' times) is continuous.\n",
    "May want to use ordinal features. \n",
    "Runners from 20 olympics would quickly turn into a large dataset.\n",
    "\n",
    "Note: falls into general category of time series forecasting (where trend over time exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. You have more features than rows in your dataset.\n",
    "Model will need to:\n",
    "* Handle overfitting\n",
    "* Reduce features \n",
    "* Computional Cost\n",
    "\n",
    "To reduce dimensionality could start with PCA or partial least squares regression (for continuous variable) as well as filtering methods for feature selection. \n",
    "Could use LASSO as embedded feature selection or ridge regression may help with multicolinearity if that reveals itself to be an issue. \n",
    "Random Forest, gradient boost, or SVM could also be used but may run into performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Identify the most important characteristic predicting the likelihood of being jailed before age 20.\n",
    "Model needs to:\n",
    "* Classifier that will produce a probability\n",
    "* Include feature importance\n",
    "\n",
    "Random forest or gradient boost classifiers could be used. These both have feature importance outputs. Logistic Regression is a great option for classifiers that we would like a probability output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement a filter to \"highlight\" emails that might be important to the recipient.\n",
    "KNN - the variables that would be used to identify that an email is important would likely be related to the sender, topic, and content of the email. There would not be a linear relationship between between any features. KNN would allow you to identify emails that are most like previous emails that have been deemed important (either through how quickly the customer opened the email or whether the customer flagged the email as important themselves after recieving). \n",
    "\n",
    "Could also use a Naive Bayes classifier like we did for spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. You have 1000+ features\n",
    "Model Needs to:\n",
    "* Avoid overfitting\n",
    "* Computationally efficient\n",
    "* Handle multicolinearity\n",
    "\n",
    "LASSO - Would push the coefficients of the features in the model that are not contributing value towards zero essentially removing features with limited contribution.\n",
    "\n",
    "Ridge - Would minimize the less important features without removing.\n",
    "\n",
    "Random Forest or Decision Tree - May help to eliminate a features that are not delivering much information content if limit put on iterations. May struggle computationally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "KNN - a grocery cart is a mix of variables that will inherently interact with eachother. I would want to let the customers past habits compare to other similar grocery carts, especially if I do not have a record of past behavior for that specific customer.\n",
    "\n",
    "Alternative a logistic regressor with a ridge penalty to control multicolinearity could be effective. I would use features that describe the customers online behavior that may indicate intent to purchase (time to purchase, number of items, frequency of purchase of similar items, times since last purchase, seasonality, item type etc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Your dataset dimensions are  982400 x 500\n",
    "Model needs to:\n",
    "* Manage high dimensionality\n",
    "* Computational Cost\n",
    "\n",
    "Start with dimensionality reduction using PCA. Could eliminate features with low variance (unless outside knowledge suggests the variable is contributing something essential to the model).\n",
    "Next you could sample this dataset if you don't have the computational power to handle it. Within that sample would still want to pull out train and test sets.\n",
    "I'm becoming a big fan of random forest for feature selection and then the final model would depend on interactions of the variables and if you were able to cut out enough features to be managable. \n",
    "Lasso/ridge would be more computationally efficient to manage data of this size than random forest, gradient boosting or SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Identify faces in an image.\n",
    "KNN - i'm assuming you have a set of labeled faces saying who the people are and you are trying to label additional faces. KNN will help identify the person who \"looks\" the closes to the trial. I'd imagine the metrics used to compare how similar the two images are would be measurementes between pigments to get at what the shapes in the photos are.\n",
    "\n",
    "I think this is usually done with Neural Networks but I'm also pretty sure Neural Networks are a type of ensemble model that layers the simpler approaches we have been discussing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Predict which of 3 flavors of ice cream will be most popular with boys vs. girls\n",
    "I'm thinking SVM? Could also take a probabalistic approach by taking percentage of each population for the 3 flavors. An A/B test could be used to support that these preferences are consistent across multiple scenerios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
