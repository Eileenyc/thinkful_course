{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2 [Tuning KNN - normalizing distance, picking k](https://courses.thinkful.com/data-201v1/project/3.1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drill:\n",
    "Imagine you work at a credit card company and are trying to predict if people will pay their bills on time.\n",
    "Data:\n",
    "* everyone's purchases\n",
    "* split into groceries, dining out, utilities & entertainment\n",
    "\n",
    "What are some ways to use KNN to create this model?\n",
    "What aspect of KNN would be useful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My hypothesis would be that the greatest predictor of whether a person will pay their credit card on time would be whether they paid their credit card the previous month.\n",
    "\n",
    "\n",
    "\n",
    "A few ways that this problem is particularly suited for KNN (and vice versa):\n",
    "* Common metric of dollars. While one type of spending may be more indicative of irresponsible spending the units are the same across the available data\n",
    "* All of the features are continuous but also bounded, there is a reasonable limit to what someone could spend in a month. \n",
    "* Whatever model we create will be classifying based on how close the behavior profile matches. This is a situation where there is likely a strong pattern for when \n",
    "* I don't know how often people miss credit card payments. Does KNN do a better job handling rare occurances/sampling imbalances than other models?\n",
    "\n",
    "\n",
    "Would try a few models with different normalizaiton techniques and transformations to the expense times to see which captured the spending habits of either a month where a bill is not paid or a person that does not pay their bills.\n",
    "1. If really all we know are absolute values of how much a person spent in these categories I would calculate percent of spend on each category to adjust the data to account for the wide variation in potential spending volume (a person could have relatively high spending and still be managing their finances responsibly. eg a person spending \\$1,000 on entertainment a month looks very different if their total monthly spending is \\$2,000 vs. \\$10,000 \n",
    "2. May want to pair dining-out & groceries since they are both food related. Or create a \"fun\" metric combining dining-out and entertainment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "\n",
    "music = pd.DataFrame()\n",
    "music['duration'] = [184, 134, 243, 186, 122, 197, 294, 382, 102, 264, \n",
    "                     205, 110, 307, 110, 397, 153, 190, 192, 210, 403,\n",
    "                     164, 198, 204, 253, 234, 190, 182, 401, 376, 102]\n",
    "music['loudness'] = [18, 34, 43, 36, 22, 9, 29, 22, 10, 24, \n",
    "                     20, 10, 17, 51, 7, 13, 19, 12, 21, 22,\n",
    "                     16, 18, 4, 23, 34, 19, 14, 11, 37, 42]\n",
    "music['jazz'] = [ 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
    "                  0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
    "                  1, 1, 1, 1, 0, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning KNN\n",
    "KNearest Neighbor can be tuned in 2 major categories:\n",
    "1. How we handle distance\n",
    "2. How many neighbors we include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance & Normalizing\n",
    "* The measurement of euclidian distance makes the assuption that all units are equal (e.g. 1 second away = to 1 loudness away)\n",
    "* In reality units are rarely equivalent\n",
    "* Makes binary or categorical variables nearly impossible to include in KNN model\n",
    "\n",
    "**Normalization** is used to make differing units of measurement more comparable. 2 main techniques are effectie with KNN:\n",
    "1. Set bounds of data to 0 and 1 then **rescale** every variable to be within thoe bounds. This is best if data shows a linear rlationship, such that scaling to a 0 to 1 range makes sense\n",
    "2. **Z-scores** Calculate how far each observation is from the mean expressed in number of standard deviations. puts everything in terms of how \"abnormal\" each datapoint is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting (by distance?)\n",
    "* Sometimes the $k$ nearest observations are not all similarly close to the test. In this case by help to weight by distance\n",
    "* Funcitonally this will weight by the inverse of distance so low distance datapoints will have a higher weight and that weight will be proportional to how much closer they are to the datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing K\n",
    "* choosing $k$ is a tradoff\n",
    "    * larger $k$ the more smoothed out your decision space will be, with more observations voting in the prediction\n",
    "    * smaller $k$ will pick up more subtl deviations but these could be noise leading to overfitting\n",
    "* Weighting will add additional dimension to the $k$ tradeoff\n",
    "\n",
    "Best technique is to try multiple models and use k-fold validation to see how KNN model is performing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
