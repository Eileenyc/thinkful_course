{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.4 [Challenge: Logistic Regressions No Penalty, Ridge, Lasso](https://courses.thinkful.com/data-201v1/project/3.3.4)\n",
    "\n",
    "Pick a dataset of your choice with a binary outcome and the potential for at least 15 features.\n",
    "\n",
    "Engineer your features, then create three models. Each model will be run on a training set and a test-set (or multiple test-sets, if you take a folds approach). The models should be:\n",
    "\n",
    "1. Vanilla logistic regression\n",
    "2. Ridge logistic regression - penalty argument L2\n",
    "3. Lasso logistic regression - penalty argument L1\n",
    "\n",
    "\n",
    "Evaluate the 3 models and select the best\n",
    "* explain why this is the best model\n",
    "* specify feature selection, regularizatio parameter selection, model evaluation criteria that led me to select my model\n",
    "* list strengths and limitations of regression as a modeling approach\n",
    "* Where there things I wished I could do that I couldn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection on Models\n",
    "I am most satisfied with the Lasso Regression. I used train and test sets before and after downsampling. For model evaluation I used the Accuracy Score. I also printed the confusion matrix but did not look closely at the results (I should have). \n",
    "I think I liked Lasso best because it was not as impacted by my poor feature selection. \n",
    "I would like to do a better job of evaluating the conribution of features. It would be great to have a pipeline that split my train/test sets, downsamples, and split train/test sets again. I also havent been including variable interaction features as much as I would like. Would be cool if I ran a loop to optimize regularization variable and was more involved in scaling/normalization of features within my model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_call = pd.read_csv('unit_3_data/telco_churn_dataset.csv')\n",
    "# Minutes/Call\n",
    "dt_call['TotalMinutes'] = dt_call['TotalDayMinutes'] + dt_call['TotalEveMinutes'] + dt_call['TotalNightMinutes']\n",
    "dt_call['MinutesPerCall'] = dt_call['TotalMinutes']/dt_call['TotalCall']\n",
    "\n",
    "# Percent of Calls at night\n",
    "dt_call['PercentNightCalls'] = dt_call['TotalNightCalls']/dt_call['TotalCall']\n",
    "dt_call['PercentDayCalls'] = dt_call['TotalDayCalls']/dt_call['TotalCall']\n",
    "\n",
    "# New subscriber flag (in first cycle of contract)\n",
    "dt_call['tenure_num'] = np.where(dt_call['tenure']==0, 0.0, \n",
    "                                 np.where(dt_call['Contract']=='One year', dt_call['tenure']/12.0,\n",
    "                                          np.where(dt_call['Contract']=='Two year', dt_call['tenure']/24.0,\n",
    "                                                   dt_call['tenure']\n",
    "                                                  )))\n",
    "dt_call['new_sub_flag'] = np.where(dt_call['tenure_num']<=1, 1, 0)\n",
    "dt_call['messages'] = np.where(dt_call['NumbervMailMessages'] >=1, 1, 0)\n",
    "\n",
    "dt_call.drop(['customerID','TotalRevenue'],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "dt_call['Churn'] = np.where(dt_call['Churn']== 'Yes',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_call = dt_call[['Churn',   \n",
    "                   'MultipleLines', \n",
    "                   'TotalIntlCalls', \n",
    "                   'TotalCall', 'CustomerServiceCalls',\n",
    "                   'tenure','MinutesPerCall', 'tenure_num', \n",
    "                   'PercentDayCalls', 'PercentNightCalls', \n",
    "                   'new_sub_flag', 'SeniorCitizen', 'InternetService',\n",
    "                   'Contract', 'PaperlessBilling', 'NumbervMailMessages' ]]\n",
    "dt_call = pd.get_dummies(dt_call, drop_first=True)\n",
    "#dt_call = pd.DataFrame(scale(dt_call), columns=dt_call.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2850\n",
       "1     483\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display class counts to see overall distribution of Churn\n",
    "dt_call.Churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n"
     ]
    }
   ],
   "source": [
    "# Identify Churn Flag\n",
    "y = dt_call.Churn\n",
    "X = dt_call.drop('Churn', axis=1)\n",
    "\n",
    "# Define the training and test sizes.\n",
    "\n",
    "train, test = train_test_split(dt_call,\n",
    "                               test_size=0.33,\n",
    "                               stratify=y                                  \n",
    ")\n",
    "\n",
    "\n",
    "# Display.\n",
    "X_holdout = test.drop('Churn',axis=1)\n",
    "y_holdout = test['Churn']\n",
    "\n",
    "# Display class counts for training set to determine what number to downsample to\n",
    "minority_samples = train.Churn.value_counts().min()\n",
    "print(minority_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    324\n",
       "0    324\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "df_majority = train[train.Churn==0]\n",
    "df_minority = train[train.Churn==1]\n",
    "\n",
    "# Downsample Majority Class\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                  replace=False,   # sample without replacement\n",
    "                                  n_samples=minority_samples,   # to match minority class\n",
    "                                  random_state=123 # reproducible results\n",
    "                                  )\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# New Class Counts\n",
    "df_downsampled.Churn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare logistic regression classifier.\n",
    "# Parameter regulariation I'm going to leave it\n",
    "lr = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_downsampled.drop('Churn', axis=1),\n",
    "                                                    df_downsampled['Churn'],\n",
    "                                                    test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "['TotalIntlCalls' 'TotalCall' 'CustomerServiceCalls' 'tenure'\n",
      " 'MinutesPerCall' 'tenure_num' 'PercentDayCalls' 'PercentNightCalls'\n",
      " 'new_sub_flag' 'SeniorCitizen' 'NumbervMailMessages' 'MultipleLines_Yes'\n",
      " 'InternetService_Fiber optic' 'InternetService_No' 'Contract_One year'\n",
      " 'Contract_Two year' 'PaperlessBilling_Yes']\n",
      "[[-0.04698645  0.00494297  0.36361792 -0.0386472   0.39980518 -0.00274286\n",
      "  -0.27780156 -0.45519936  0.54649369  0.04243869 -0.02606389 -1.80094948\n",
      "   1.31605354 -0.87387106 -0.72187925 -1.88231894  0.53886703]]\n",
      "[-1.99031778]\n",
      "\n",
      " In Sample Accuracy by admission status\n",
      "Churn    0    1\n",
      "row_0          \n",
      "0      159   28\n",
      "1       47  200\n",
      "\n",
      " In Sample Percentage Accuracy\n",
      "0.8271889400921659\n",
      "\n",
      " Out of Sample Accuracy by admission status\n",
      "Churn   0   1\n",
      "row_0        \n",
      "0      83  13\n",
      "1      35  83\n",
      "\n",
      " Out Sample Percentage Accuracy\n",
      "0.7757009345794392\n",
      "\n",
      " Original Test Sample Accuracy by admission status\n",
      "Churn    0    1\n",
      "row_0          \n",
      "0      711   25\n",
      "1      230  134\n",
      "\n",
      " Original Test Sample Percentage Accuracy\n",
      "0.7681818181818182\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to training set\n",
    "fit = lr.fit(X_train, y_train)\n",
    "\n",
    "# Display.\n",
    "print('Coefficients')\n",
    "print(X_train.columns.values)\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "\n",
    "# In Sample Predictions\n",
    "pred_y_train = lr.predict(X_train)\n",
    "\n",
    "print('\\n In Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_train, y_train))\n",
    "\n",
    "print('\\n In Sample Percentage Accuracy')\n",
    "print(lr.score(X_train, y_train))\n",
    "\n",
    "\n",
    "\n",
    "# Out of Sample Predictions\n",
    "pred_y_test = lr.predict(X_test)\n",
    "\n",
    "print('\\n Out of Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_test, y_test))\n",
    "\n",
    "print('\\n Out Sample Percentage Accuracy')\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Original holdout Sample Predictions\n",
    "pred_y_holdout = lr.predict(X_holdout)\n",
    "\n",
    "print('\\n Original Test Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_holdout, y_holdout))\n",
    "\n",
    "print('\\n Original Test Sample Percentage Accuracy')\n",
    "print(lr.score(X_holdout, y_holdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Ridge Regression Model\n",
    "# In Sklearn Alpha is the regularization parameter\n",
    "# As alpha gets larger, parameter shrinkage becomes more pronounce. \n",
    "# Intercept is not regularized\n",
    "\n",
    "# In the example the data is scaled\n",
    "\n",
    "# Fit ridgeregr\n",
    "ridge = LogisticRegression(penalty='l2', C=10, fit_intercept=True)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "['TotalIntlCalls' 'TotalCall' 'CustomerServiceCalls' 'tenure'\n",
      " 'MinutesPerCall' 'tenure_num' 'PercentDayCalls' 'PercentNightCalls'\n",
      " 'new_sub_flag' 'SeniorCitizen' 'NumbervMailMessages' 'MultipleLines_Yes'\n",
      " 'InternetService_Fiber optic' 'InternetService_No' 'Contract_One year'\n",
      " 'Contract_Two year' 'PaperlessBilling_Yes']\n",
      "[[-0.04698645  0.00494297  0.36361792 -0.0386472   0.39980518 -0.00274286\n",
      "  -0.27780156 -0.45519936  0.54649369  0.04243869 -0.02606389 -1.80094948\n",
      "   1.31605354 -0.87387106 -0.72187925 -1.88231894  0.53886703]]\n",
      "[-1.99031778]\n",
      "\n",
      " In Sample Accuracy by admission status\n",
      "Churn    0    1\n",
      "row_0          \n",
      "0      164   33\n",
      "1       42  195\n",
      "\n",
      " In Sample Percentage Accuracy\n",
      "0.8271889400921659\n",
      "\n",
      " Out of Sample Accuracy by admission status\n",
      "Churn   0   1\n",
      "row_0        \n",
      "0      91  14\n",
      "1      27  82\n",
      "\n",
      " Out Sample Percentage Accuracy\n",
      "0.8084112149532711\n",
      "\n",
      " Original Test Sample Accuracy by admission status\n",
      "Churn    0    1\n",
      "row_0          \n",
      "0      715   23\n",
      "1      226  136\n",
      "\n",
      " Original Test Sample Percentage Accuracy\n",
      "0.7736363636363637\n"
     ]
    }
   ],
   "source": [
    "# Fit Ridge Regression Model\n",
    "# In Sklearn Alpha is the regularization parameter\n",
    "# As alpha gets larger, parameter shrinkage becomes more pronounce. \n",
    "# Intercept is not regularized\n",
    "\n",
    "# In the example the data is scaled\n",
    "\n",
    "# Fit ridgeregr\n",
    "ridge = LogisticRegression(penalty='l2', C=10, fit_intercept=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Display.\n",
    "print('Coefficients')\n",
    "print(X_train.columns.values)\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "\n",
    "# In Sample Predictions\n",
    "pred_y_train = ridge.predict(X_train)\n",
    "\n",
    "print('\\n In Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_train, y_train))\n",
    "\n",
    "print('\\n In Sample Percentage Accuracy')\n",
    "print(ridge.score(X_train, y_train))\n",
    "\n",
    "\n",
    "# Out of Sample Predictions\n",
    "pred_y_test = ridge.predict(X_test)\n",
    "\n",
    "print('\\n Out of Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_test, y_test))\n",
    "\n",
    "print('\\n Out Sample Percentage Accuracy')\n",
    "print(ridge.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Original Holdout Test Set\n",
    "pred_y_holdout = ridge.predict(X_holdout)\n",
    "\n",
    "print('\\n Original Test Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_holdout, y_holdout))\n",
    "\n",
    "print('\\n Original Test Sample Percentage Accuracy')\n",
    "print(ridge.score(X_holdout, y_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "['TotalIntlCalls' 'TotalCall' 'CustomerServiceCalls' 'tenure'\n",
      " 'MinutesPerCall' 'tenure_num' 'PercentDayCalls' 'PercentNightCalls'\n",
      " 'new_sub_flag' 'SeniorCitizen' 'NumbervMailMessages' 'MultipleLines_Yes'\n",
      " 'InternetService_Fiber optic' 'InternetService_No' 'Contract_One year'\n",
      " 'Contract_Two year' 'PaperlessBilling_Yes']\n",
      "[[-0.01961952 -0.00281472  0.41899425 -0.02523088  1.15843322 -0.00260357\n",
      "  -0.03780221 -0.07143994  0.56389646  0.77026105 -0.02493072 -2.06089247\n",
      "   0.51320985 -1.31984186 -1.05342253 -1.86660257  0.06812434]]\n",
      "[-1.06007715]\n",
      "\n",
      " In Sample Accuracy by admission status\n",
      "Churn    0    1\n",
      "row_0          \n",
      "0      162   24\n",
      "1       44  204\n",
      "\n",
      " In Sample Percentage Accuracy\n",
      "0.8433179723502304\n",
      "\n",
      " Out of Sample Accuracy by admission status\n",
      "Churn   0   1\n",
      "row_0        \n",
      "0      90  14\n",
      "1      28  82\n",
      "\n",
      " Out Sample Percentage Accuracy\n",
      "0.8037383177570093\n",
      "\n",
      " Original Test Sample Accuracy by admission status\n",
      "Churn    0    1\n",
      "row_0          \n",
      "0      699   16\n",
      "1      242  143\n",
      "\n",
      " Original Test Sample Percentage Accuracy\n",
      "0.7654545454545455\n"
     ]
    }
   ],
   "source": [
    "# Fit Ridge Regression Model\n",
    "# In Sklearn Alpha is the regularization parameter\n",
    "# As alpha gets larger, parameter shrinkage becomes more pronounce. \n",
    "# Intercept is not regularized\n",
    "\n",
    "# In the example the data is scaled\n",
    "\n",
    "# Fit ridgeregr\n",
    "ridge = LogisticRegression(penalty='l2', C=20, fit_intercept=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Display.\n",
    "print('Coefficients')\n",
    "print(X_train.columns.values)\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "\n",
    "# In Sample Predictions\n",
    "pred_y_train = ridge.predict(X_train)\n",
    "\n",
    "print('\\n In Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_train, y_train))\n",
    "\n",
    "print('\\n In Sample Percentage Accuracy')\n",
    "print(ridge.score(X_train, y_train))\n",
    "\n",
    "\n",
    "\n",
    "# Out of Sample Predictions\n",
    "pred_y_test = ridge.predict(X_test)\n",
    "\n",
    "print('\\n Out of Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_test, y_test))\n",
    "\n",
    "print('\\n Out Sample Percentage Accuracy')\n",
    "print(ridge.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "# Original Hold out TestPredictions\n",
    "pred_y_holdout = ridge.predict(X_holdout)\n",
    "\n",
    "print('\\n Original Test Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_holdout, y_holdout))\n",
    "\n",
    "print('\\n Original Test Sample Percentage Accuracy')\n",
    "print(ridge.score(X_holdout, y_holdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "['TotalIntlCalls' 'TotalCall' 'CustomerServiceCalls' 'tenure'\n",
      " 'MinutesPerCall' 'tenure_num' 'PercentDayCalls' 'PercentNightCalls'\n",
      " 'new_sub_flag' 'SeniorCitizen' 'NumbervMailMessages' 'MultipleLines_Yes'\n",
      " 'InternetService_Fiber optic' 'InternetService_No' 'Contract_One year'\n",
      " 'Contract_Two year' 'PaperlessBilling_Yes']\n",
      "[[-0.04698645  0.00494297  0.36361792 -0.0386472   0.39980518 -0.00274286\n",
      "  -0.27780156 -0.45519936  0.54649369  0.04243869 -0.02606389 -1.80094948\n",
      "   1.31605354 -0.87387106 -0.72187925 -1.88231894  0.53886703]]\n",
      "[-1.99031778]\n",
      "\n",
      " In Sample Accuracy by admission status\n",
      "Churn    0    1\n",
      "row_0          \n",
      "0      165   32\n",
      "1       41  196\n",
      "\n",
      " In Sample Percentage Accuracy\n",
      "0.8317972350230415\n",
      "\n",
      " Out of Sample Accuracy by admission status\n",
      "Churn   0   1\n",
      "row_0        \n",
      "0      91  15\n",
      "1      27  81\n",
      "\n",
      " Out Sample Percentage Accuracy\n",
      "0.8037383177570093\n",
      "\n",
      " Original Test Sample Accuracy by admission status\n",
      "Churn    0    1\n",
      "row_0          \n",
      "0      720   25\n",
      "1      221  134\n",
      "\n",
      " Original Test Sample Percentage Accuracy\n",
      "0.7763636363636364\n"
     ]
    }
   ],
   "source": [
    "# Declare \n",
    "# Fit Ridge Regression Model\n",
    "# In Sklearn Alpha is the regularization parameter\n",
    "# As alpha gets larger, parameter shrinkage becomes more pronounce. \n",
    "# Intercept is not regularized\n",
    "\n",
    "# In the example the data is scaled\n",
    "\n",
    "# Fit ridgeregr\n",
    "ridge = LogisticRegression(penalty='l1', C=4, fit_intercept=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Display.\n",
    "print('Coefficients')\n",
    "print(X_train.columns.values)\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "\n",
    "# In Sample Predictions\n",
    "pred_y_train = ridge.predict(X_train)\n",
    "\n",
    "print('\\n In Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_train, y_train))\n",
    "\n",
    "print('\\n In Sample Percentage Accuracy')\n",
    "print(ridge.score(X_train, y_train))\n",
    "\n",
    "\n",
    "\n",
    "# Out of Sample Predictions\n",
    "pred_y_test = ridge.predict(X_test)\n",
    "\n",
    "print('\\n Out of Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_test, y_test))\n",
    "\n",
    "print('\\n Out Sample Percentage Accuracy')\n",
    "print(ridge.score(X_test, y_test))\n",
    "\n",
    "# Display.\n",
    "X_holdout = test.drop('Churn',axis=1)\n",
    "y_holdout = test['Churn']\n",
    "\n",
    "# Original Hold out TestPredictions\n",
    "pred_y_holdout = ridge.predict(X_holdout)\n",
    "\n",
    "print('\\n Original Test Sample Accuracy by admission status')\n",
    "print(pd.crosstab(pred_y_holdout, y_holdout))\n",
    "\n",
    "print('\\n Original Test Sample Percentage Accuracy')\n",
    "print(ridge.score(X_holdout, y_holdout))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
